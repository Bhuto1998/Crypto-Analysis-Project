mplications for how to solve for exact solutions.
Sometimes dilution is used for adding damping noise to the inputs. In that case, weak dilution refers to adding a small amount of damping noise, while strong dilution refers to adding a greater amount of damping noise. Both can be rewritten as variants of weight dilution.
These techniques are also sometimes referred to as random pruning of weights, but this is usually a non-recurring one-way operation. The network is pruned, and then kept if it is an improvement over the previous model. Dilution and dropout both refer to an iterative process. The pruning of weights typically does not imply that the network continues learning, while in dilution/dropout, the network continues to learn after the technique is applied.


== Generalized linear network ==
Output from a layer of linear nodes, in an artificial neural net can be described as

  
    
      
        
          y
          
            i
          
        
      
    
    {\displaystyle y_{i}}
   â€“ outpu